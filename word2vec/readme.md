# word2vec

Известная утилита `word2vec`, основанная на нейронной сети, умеет приписывать каждому слову из словаря некоторый числовой вектор размерности `N`. При этом оказывается, что семантически ''близким'' словам соответствуют ''близкие'' векторы. Эта утилита находит широкое применение в задачах обработки естественного языка и машинного обучения.

В качестве меры ''близости'' между словами обычно выбирают угол между их векторами, косинус которого вычисляют по известной формуле:

![equation](https://latex.codecogs.com/gif.latex?\cos&space;\alpha&space;=&space;\frac{\langle&space;u,&space;v&space;\rangle}{\sqrt{\langle&space;u,&space;u&space;\rangle}&space;\sqrt{\langle&space;v,&space;v&space;\rangle}})

Однако часто векторы слов нормируют (чтобы их длина стала равна константе), и тогда в качестве меры близости рассматривают просто скалярное произведение:

![equation](https://latex.codecogs.com/gif.latex?\langle&space;u,&space;v&space;\rangle&space;=&space;\sum_{i&space;=&space;1}^N&space;u_i&space;v_i)

Чем это скалярное произведение больше, тем семантически ''ближе'' слова друг к другу.

Ваша задача - написать функцию `ClosestWords`. В функцию передаются:

- список из `M` слов
- список из `M` соответствующих им `N`-мерных векторов

Гарантируется, что все слова различны, а размерности векторов равны между собой. Требуется найти в этом списке ближайшие слова к первому слову (кроме самого этого первого слова), используя в качестве метрики близости просто скалярное произведение.

Функция должна возвращать список слов, скалярное произведение векторов которых с вектором первого слова максимально. При этом скалярное произведение первого слова с самим собой учитывать не нужно. Если таких слов несколько, то в возвращаемом списке они должны следовать в том же порядке, в котором были даны на входе.
